import sys, os
sys.dont_write_bytecode = True

import time
import json
import os
from datetime import datetime

try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    # If dotenv not available, use environment variables directly
    def load_dotenv():
        pass

import pandas as pd
import streamlit as st
import openai
from streamlit_modal import Modal

from langchain_core.messages import AIMessage, HumanMessage
from langchain_community.vectorstores import FAISS
from langchain_community.vectorstores.faiss import DistanceStrategy
from langchain_community.embeddings import HuggingFaceEmbeddings

from llm_agent import ChatBot
from ingest_data import ingest
from retriever import SelfQueryRetriever
import chatbot_verbosity as chatbot_verbosity

# Page config MUST be first Streamlit command
st.set_page_config(
    page_title="AI Resume Screener Pro",
    page_icon="ğŸ¯",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Load environment variables (dotenv is optional)
load_dotenv()

DATA_PATH = os.getenv("DATA_PATH", "data/main-data/synthetic-resumes.csv")
FAISS_PATH = os.getenv("FAISS_PATH", "vectorstore")
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "sentence-transformers/all-MiniLM-L6-v2")

# Custom CSS for modern UI
st.markdown("""
<style>
    /* Main styling */
    .main {
        padding: 2rem 1rem;
    }
    
    /* Header styling */
    .header-container {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 10px;
        margin-bottom: 2rem;
        color: white;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }
    
    .header-title {
        font-size: 2.5rem;
        font-weight: 700;
        margin-bottom: 0.5rem;
    }
    
    .header-subtitle {
        font-size: 1.1rem;
        opacity: 0.9;
    }
    
    /* Stats cards */
    .stats-container {
        display: flex;
        gap: 1rem;
        margin-bottom: 2rem;
    }
    
    .stat-card {
        background: white;
        padding: 1.5rem;
        border-radius: 10px;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        flex: 1;
        border-left: 4px solid #667eea;
    }
    
    .stat-value {
        font-size: 2rem;
        font-weight: 700;
        color: #667eea;
    }
    
    .stat-label {
        font-size: 0.9rem;
        color: #666;
        margin-top: 0.5rem;
    }
    
    /* Chat message styling */
    .stChatMessage {
        padding: 1rem;
        border-radius: 10px;
        margin-bottom: 1rem;
    }
    
    /* Button styling */
    .stButton > button {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        border: none;
        border-radius: 5px;
        padding: 0.5rem 1.5rem;
        font-weight: 600;
        transition: all 0.3s;
    }
    
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
    }
    
    /* Sidebar styling */
    .sidebar .sidebar-content {
        background: #f8f9fa;
    }
    
    /* Info boxes */
    .info-box {
        background: #e3f2fd;
        padding: 1rem;
        border-radius: 5px;
        border-left: 4px solid #2196f3;
        margin: 1rem 0;
    }
    
    /* Success message */
    .success-box {
        background: #e8f5e9;
        padding: 1rem;
        border-radius: 5px;
        border-left: 4px solid #4caf50;
        margin: 1rem 0;
    }
</style>
""", unsafe_allow_html=True)

# Welcome message
welcome_message = """
### ğŸ¯ Welcome to AI Resume Screener Pro

**An intelligent RAG-powered system for efficient candidate screening**

This advanced system uses Retrieval-Augmented Generation (RAG) to help you find the best candidates from thousands of resumes. 

#### ğŸš€ Quick Start

1. **Add your OpenAI API key** in the sidebar ğŸ”‘
2. **Upload resumes** or use the default dataset ğŸ“„
3. **Enter a job description** to find matching candidates ğŸ’¼
4. **Get AI-powered insights** and candidate recommendations ğŸ¤–

#### âœ¨ Key Features

- **Smart Retrieval**: Uses RAG Fusion for better candidate matching
- **Real-time Analysis**: Get instant insights on candidate fit
- **Export Results**: Download candidate lists and reports
- **Analytics Dashboard**: View statistics and trends
- **Multi-Model Support**: Works with GPT-4, GPT-3.5, and more

#### ğŸ’¡ Pro Tips

- Be specific in your job descriptions for better matches
- Use the analytics tab to understand your candidate pool
- Export results for further analysis in Excel/CSV
"""

info_message = """
### ğŸ“š Documentation

#### ğŸ“¤ Uploading Resumes

Your CSV file must contain two columns:
- **ID**: Unique identifier for each candidate
- **Resume**: Full text content of the resume

The system will automatically index your resumes for fast retrieval.

#### âš™ï¸ Configuration

- **RAG Mode**: Choose between Generic RAG or RAG Fusion
  - Generic RAG: Faster, good for simple queries
  - RAG Fusion: Better for complex, multi-faceted job descriptions
  
- **Model Selection**: Choose your preferred OpenAI model
  - Recommended: gpt-4o-mini (cost-effective)
  - Best quality: gpt-4 or gpt-4-turbo

#### ğŸ”’ Privacy & Security

- All processing happens in real-time
- No data is permanently stored
- Your API key is only used for LLM calls
- Uploaded files are processed in memory only

#### ğŸ“ How It Works

1. **Query Processing**: Your job description is analyzed
2. **Semantic Search**: Resumes are matched using embeddings
3. **RAG Fusion** (if enabled): Multiple query perspectives improve results
4. **LLM Analysis**: AI provides detailed candidate insights
5. **Ranking**: Candidates are ranked by relevance
"""

about_message = """
### ğŸ¯ About AI Resume Screener Pro

**Version 2.0** - Enhanced RAG Pipeline for Intelligent Resume Screening

This system leverages state-of-the-art language models and semantic search to revolutionize the resume screening process. Built with:

- **LangChain** for RAG pipeline orchestration
- **FAISS** for efficient vector similarity search
- **OpenAI GPT** for intelligent analysis
- **Streamlit** for intuitive user interface

#### ğŸš€ What Makes This Different

- **Advanced RAG Fusion**: Better candidate matching through multi-perspective queries
- **Real-time Analytics**: Instant insights into your candidate pool
- **Export Capabilities**: Download results for further analysis
- **Modern UI**: Clean, professional interface designed for productivity
- **Scalable Architecture**: Handles thousands of resumes efficiently

#### ğŸ“Š Performance

- **Speed**: Processes queries in seconds
- **Accuracy**: High precision candidate matching
- **Scalability**: Tested with 10,000+ resumes
"""

# Initialize session state - make everything lazy to avoid blocking startup
if "chat_history" not in st.session_state:
    st.session_state.chat_history = [AIMessage(content=welcome_message)]

# Don't load data on startup - only when needed
if "df" not in st.session_state:
    st.session_state.df = None
    st.session_state.data_loaded = False

# Lazy load embedding model - only when needed (not on startup)
if "embedding_model" not in st.session_state:
    st.session_state.embedding_model = None

# Don't load vectorstore on startup - completely lazy
if "rag_pipeline" not in st.session_state:
    st.session_state.rag_pipeline = None

if "resume_list" not in st.session_state:
    st.session_state.resume_list = []

if "query_stats" not in st.session_state:
    st.session_state.query_stats = {
        "total_queries": 0,
        "total_candidates_found": 0,
        "avg_response_time": 0,
        "queries_today": []
    }

# Header
st.markdown("""
<div class="header-container">
    <div class="header-title">ğŸ¯ AI Resume Screener Pro</div>
    <div class="header-subtitle">Intelligent Candidate Matching with RAG Technology</div>
</div>
""", unsafe_allow_html=True)

# Stats dashboard
col1, col2, col3, col4 = st.columns(4)
with col1:
    resume_count = len(st.session_state.df) if st.session_state.df is not None else 0
    st.metric("ğŸ“Š Total Resumes", resume_count)
with col2:
    st.metric("ğŸ” Queries Today", len(st.session_state.query_stats["queries_today"]))
with col3:
    st.metric("âš¡ Avg Response", f"{st.session_state.query_stats['avg_response_time']:.1f}s")
with col4:
    st.metric("âœ… Candidates Found", st.session_state.query_stats["total_candidates_found"])

st.divider()

# Sidebar
with st.sidebar:
    st.markdown("### âš™ï¸ Control Panel")
    
    # API Key
    api_key = st.text_input(
        "ğŸ”‘ OpenAI API Key", 
        type="password", 
        key="api_key",
        help="Enter your OpenAI API key to enable AI features"
    )
    
    # Configuration
    st.markdown("#### ğŸ›ï¸ Configuration")
    rag_mode = st.selectbox(
        "RAG Mode", 
        ["Generic RAG", "RAG Fusion"], 
        index=0,
        key="rag_selection",
        help="RAG Fusion provides better results for complex queries"
    )
    
    gpt_model = st.text_input(
        "ğŸ¤– GPT Model", 
        value="gpt-4o-mini",
        key="gpt_selection",
        help="Recommended: gpt-4o-mini, gpt-4, or gpt-3.5-turbo"
    )
    
    # File upload
    st.markdown("#### ğŸ“¤ Data Management")
    uploaded_file = st.file_uploader(
        "Upload Resumes (CSV)", 
        type=["csv"], 
        key="uploaded_file",
        help="Upload a CSV file with 'ID' and 'Resume' columns"
    )
    
    if uploaded_file is not None:
        try:
            df_load = pd.read_csv(uploaded_file)
            if "Resume" in df_load.columns and "ID" in df_load.columns:
                # Lazy load embedding model only when needed
                if st.session_state.embedding_model is None:
                    with st.spinner("ğŸ”„ Loading embedding model (first time only)... This may take 1-2 minutes."):
                        try:
                            st.session_state.embedding_model = HuggingFaceEmbeddings(
                                model_name=EMBEDDING_MODEL, 
                                model_kwargs={"device": "cpu"}
                            )
                        except Exception as e:
                            st.error(f"âŒ Failed to load embedding model: {str(e)}")
                            st.stop()
                with st.spinner("ğŸ”„ Indexing resumes... This may take a moment."):
                    st.session_state.df = df_load
                    st.session_state.data_loaded = True
                    vectordb = ingest(st.session_state.df, "Resume", st.session_state.embedding_model)
                    st.session_state.rag_pipeline = SelfQueryRetriever(vectordb, st.session_state.df)
                st.success(f"âœ… Successfully loaded and indexed {len(df_load)} resumes!")
                st.rerun()
            else:
                st.error("âŒ CSV must contain 'ID' and 'Resume' columns")
        except Exception as e:
            st.error(f"âŒ Error loading file: {str(e)}")
    
    # Actions
    st.markdown("#### ğŸ› ï¸ Actions")
    if st.button("ğŸ—‘ï¸ Clear Conversation", use_container_width=True):
        st.session_state.chat_history = [AIMessage(content=welcome_message)]
        st.session_state.resume_list = []
        st.rerun()
    
    # Export results
    if st.session_state.resume_list:
        st.markdown("#### ğŸ“¥ Export Results")
        export_data = {
            "query": st.session_state.chat_history[-2].content if len(st.session_state.chat_history) > 1 else "",
            "candidates": st.session_state.resume_list,
            "timestamp": datetime.now().isoformat(),
            "rag_mode": rag_mode,
            "model": gpt_model
        }
        st.download_button(
            "ğŸ’¾ Download Results (JSON)",
            data=json.dumps(export_data, indent=2),
            file_name=f"resume_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
            mime="application/json",
            use_container_width=True
        )
    
    st.divider()
    
    # Info tabs
    tab1, tab2, tab3 = st.tabs(["ğŸ“– Info", "ğŸ“š Docs", "â„¹ï¸ About"])
    
    with tab1:
        st.markdown(info_message)
    
    with tab2:
        st.markdown("""
        ### ğŸ“š User Guide
        
        **Getting Started:**
        1. Add your OpenAI API key
        2. Upload resumes or use default dataset
        3. Enter job description
        4. Review AI recommendations
        
        **Best Practices:**
        - Use detailed job descriptions
        - Include required skills and experience
        - Specify company culture fit if relevant
        - Review multiple candidates for comparison
        """)
    
    with tab3:
        st.markdown(about_message)
        st.markdown("---")
        st.markdown("**Built with â¤ï¸ using RAG Technology**")

# Helper functions
def check_openai_api_key(api_key: str):
    if not api_key:
        return False
    openai.api_key = api_key
    try:
        _ = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": "Hello!"}],
            max_tokens=3
        )
        return True
    except:
        return False

def check_model_name(model_name: str, api_key: str):
    if not api_key:
        return False
    openai.api_key = api_key
    try:
        model_list = [model.id for model in openai.models.list()]
        return model_name in model_list
    except:
        return False

# Main chat interface
st.markdown("### ğŸ’¬ Chat with AI Resume Screener")

# Display chat history
for message in st.session_state.chat_history:
    if isinstance(message, AIMessage):
        with st.chat_message("assistant"):
            st.markdown(message.content)
    elif isinstance(message, HumanMessage):
        with st.chat_message("user"):
            st.markdown(message.content)
    else:
        with st.chat_message("assistant"):
            message[0].render(*message[1:])

# API key validation
if not st.session_state.api_key:
    st.info("ğŸ”‘ Please add your OpenAI API key in the sidebar to continue.")
    st.stop()

if not check_openai_api_key(st.session_state.api_key):
    st.error("âŒ Invalid API key. Please check your OpenAI API key.")
    st.stop()

if not check_model_name(st.session_state.gpt_selection, st.session_state.api_key):
    st.error("âŒ Invalid model name. Please check available models.")
    st.stop()

# Check if data is loaded
if st.session_state.df is None or len(st.session_state.df) == 0:
    st.info("ğŸ“¤ **Welcome!** Please upload a CSV file with resumes (ID and Resume columns) to get started.")
    st.stop()

# Check if vectorstore is available
if st.session_state.rag_pipeline is None:
    st.warning("âš ï¸ **No resume data indexed yet.** Please upload a CSV file to index your resumes.")
    st.stop()

# Initialize retriever and LLM
retriever = st.session_state.rag_pipeline
llm = ChatBot(
    api_key=st.session_state.api_key,
    model=st.session_state.gpt_selection,
)

# Chat input
user_query = st.chat_input("ğŸ’¬ Enter a job description or ask about candidates...")

if user_query and user_query.strip():
    # Add user message
    with st.chat_message("user"):
        st.markdown(user_query)
    st.session_state.chat_history.append(HumanMessage(content=user_query))
    
    # Generate response
    with st.chat_message("assistant"):
        start_time = time.time()
        with st.spinner("ğŸ¤– Analyzing candidates..."):
            try:
                document_list = retriever.retrieve_docs(
                    user_query, 
                    llm, 
                    st.session_state.rag_selection
                )
                query_type = retriever.meta_data["query_type"]
                st.session_state.resume_list = document_list
                stream_message = llm.generate_message_stream(
                    user_query, 
                    document_list, 
                    [], 
                    query_type
                )
            except Exception as e:
                st.error(f"âŒ Error: {str(e)}")
                st.stop()
        
        end_time = time.time()
        response_time = end_time - start_time
        
        # Display response
        response = st.write_stream(stream_message)
        
        # Display retriever details
        retriever_message = chatbot_verbosity
        retriever_message.render(
            document_list, 
            retriever.meta_data, 
            response_time
        )
        
        # Update stats
        st.session_state.query_stats["total_queries"] += 1
        st.session_state.query_stats["total_candidates_found"] += len(document_list)
        st.session_state.query_stats["queries_today"].append({
            "time": datetime.now().isoformat(),
            "query": user_query,
            "candidates": len(document_list)
        })
        
        # Update average response time
        total_time = st.session_state.query_stats["avg_response_time"] * (st.session_state.query_stats["total_queries"] - 1)
        st.session_state.query_stats["avg_response_time"] = (total_time + response_time) / st.session_state.query_stats["total_queries"]
        
        # Add to chat history
        st.session_state.chat_history.append(AIMessage(content=response))
        st.session_state.chat_history.append((
            retriever_message, 
            document_list, 
            retriever.meta_data, 
            response_time
        ))

# Analytics section (if there are results)
if st.session_state.resume_list and len(st.session_state.chat_history) > 0:
    # Get last response time from chat history
    last_response_time = 0
    for msg in reversed(st.session_state.chat_history):
        if isinstance(msg, tuple) and len(msg) > 3:
            last_response_time = msg[3]
            break
    
    st.divider()
    st.markdown("### ğŸ“Š Candidate Analysis")
    
    col1, col2 = st.columns(2)
    with col1:
        st.metric("ğŸ¯ Candidates Found", len(st.session_state.resume_list))
    with col2:
        if last_response_time > 0:
            st.metric("â±ï¸ Response Time", f"{last_response_time:.2f}s")
    
    # Display candidate previews
    with st.expander("ğŸ‘¥ View Candidate Details", expanded=False):
        for i, candidate in enumerate(st.session_state.resume_list[:5], 1):
            st.markdown(f"**Candidate {i}**")
            st.text_area("", candidate, height=150, key=f"candidate_{i}", disabled=True)
            st.divider()

